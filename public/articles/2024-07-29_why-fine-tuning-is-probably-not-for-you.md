---
title: Why Fine-Tuning is (Probably) Not for You
author: Sam Keen
date: July 29, 2024
url: https://devthink.ai/p/why-fine-tuning-is-probably-not-for-you
scraped_at: 2025-07-29T19:22:37.598190
---

# Why Fine-Tuning is (Probably) Not for You

*By Sam Keen on July 29, 2024*

---

### **Essential AI Content for Software Devs,** **Minus the Hype**

Another jam-packed edition of our newsletter is here, filled with insights that will help you stay ahead of the curve in the world of generative AI. This week, we explore cost-effective techniques for enhancing large language models, dive into how Mantle used LLMs to streamline code conversion, and unpack the pros and cons of fine-tuning versus Retrieval Augmented Generation. Let's dive in!



## ðŸ“–Â **TUTORIALS & CASE STUDIES**

### **Generating Powerful LLMs from Scratch: A Cost-Effective Approach**

Read Time: 12 minutes



This article exploresÂ severalÂ [cutting-edge techniques to enhance LLMs]("https://www.linkedin.com/pulse/instruction-pretraining-llms-sebastian-raschka-phd-x6zoc/")Â for software developers.Â It covers a novel method called "Magpie" that can generate high-quality instruction datasets for LLM finetuning usingÂ just a local Llama 3 8B model.Â It also examines "Instruction Pretraining",Â a technique that improves LLM performance byÂ incorporating synthetic instruction-response pairs during pretraining.Â Finally,Â it provides an in-depth overview ofÂ Google's new Gemma 2 models,Â highlighting their architectural innovations for creating efficient and capable LLMs.

### **Working with AI (Part 2): Code Conversion**

Read time: 9 minutes



[This article]("https://blog.withmantle.com/code-conversion-using-ai/")Â explores how Mantle used large language models ( LLMs) to streamline code conversion from prototype to production,Â saving two-thirds of the development time.Â ByÂ leveraging LLM capabilities to generate code with context,Â such as existing code patterns,Â libraries,Â and visualÂ references,Â the team was able to rapidly create production-ready code.Â As token windows continue to grow,Â this approachÂ demonstrates the potential for LLMs to drive more efficient and higher-quality software development.

### **Why Fine-Tuning is (Probably) Not for You**

Read time: 11 minutes



[This article]("https://blog.promptlayer.com/why-fine-tuning-is-probably-not-for-you-37d8c4987530")Â examines the pros andÂ cons of fine-tuning generative AI models versus using Retrieval Augmented Generation (RAG).Â It suggests that for mostÂ software developers,Â RAG often outperforms fine-tuning while being less complex,Â faster to iterate,Â and moreÂ cost-effective.Â The article highlights use cases where fine-tuning may still be beneficial,Â such as for specific outputÂ formats or editing writing style,Â but overall recommends focusing on prompting and RAG for faster development and lowerÂ costs.

### **Building A Generative AI Platform**

Read time: 30 minutes



This article provides a comprehensiveÂ [overview of the common components and architecture of a Generative AI platform]("https://huyenchip.com/2024/07/25/genai-platform.html").Â ItÂ covers key steps such as enhancing context withÂ Retrieval-Augmented Generation (RAG),Â implementing guardrails to ensure reliability,Â adding model routers and gatewaysÂ for security and scalability,Â leveraging caching techniques to reduce latency,Â and incorporating complex logic and writeÂ actions.Â The article also discusses the importance of observability and AI pipeline orchestration,Â making it a valuableÂ resource for software developers looking to leverage GenAI in their applications.

##

## ðŸ§°Â **TOOLS**

### **Run Your Own AI Cluster at Home with Everyday Devices**

Read time: 8 minutes



[exo]("https://github.com/exo-explore/exo")Â is an experimental open-source framework that allows software developers toÂ run powerful AI models like LLaMA on a cluster of their own devices,Â including smartphones,Â laptops,Â and desktops.Â exoÂ features dynamic model partitioning,Â automatic device discovery,Â and a ChatGPT-compatible API,Â enabling developers toÂ leverage large-scale generative AI in their applications without expensive specialized hardware.

### **PraisonAI: A Low-Code Solution for Building Multi-Agent LLM Systems**

Read time: 8 minutes



[PraisonAI]("https://github.com/MervinPraison/PraisonAI")Â is a low-code,Â centralized framework that simplifies theÂ creation and orchestration of multi-agent systems for various LLM applications.Â It leverages both AutoGen and CrewAIÂ frameworks,Â emphasizing ease of use,Â customization,Â and efficient human-agent collaboration.Â PraisonAI offers differentÂ user interfaces,Â including a multi-agent UI,Â a chat interface for 100+Â LLMs,Â and a "chat with your entire codebase" feature,Â making it a versatile tool for software developers seeking to leverage generative AI in their applications.

### **MambaVision: A Hybrid Mamba-Transformer Vision Backbone**

Read Time: 5 minutes



[MambaVision]("https://github.com/NVlabs/MambaVision")Â is a new PyTorch-based vision backbone that combines the strengthsÂ of Mamba and Transformer models.Â This hybrid approach aims to improve performance on various computer vision tasks.Â Developers interested in leveraging cutting-edge AI models in their applications will find this official implementationÂ from NVIDIA Labs valuable for exploring the capabilities of MambaVision and potentially integrating it into theirÂ projects.

### **llama-agentic-system: Leveraging Agentic Components of the Llama Stack**

Read time: 10 minutes



This GitHub repo introduces theÂ [llama-agentic-system]("https://github.com/meta-llama/llama-agentic-system"),Â a frameworkÂ that allows running Llama 3.1 as an "agentic" system capable of multi-step reasoning,Â tool usage,Â and safety-focusedÂ configuration.Â The system enables software developers to leverage Llama's capabilities in their applications,Â withÂ features like built-in and zero-shot tool integration,Â as well as customizable safety shields like Llama Guard.Â The repoÂ provides detailed installation,Â setup,Â and usage guides to help developers get started.

### **Open-Source Python Toolkit for Ordinal Deep Learning**

Read Time: 6 minutes

[dlordinal]("https://github.com/ayrna/dlordinal")Â is an open-source Python library that provides a unified toolkit forÂ deep learning with ordinal methodologies.Â Developed using PyTorch,Â it implements state-of-the-art techniques for ordinalÂ classification problems,Â which leverage the ordering information in target variables.Â The library includes lossÂ functions,Â output layers,Â dropout techniques,Â soft labeling,Â and ordinal evaluation metrics,Â all designed to handleÂ ordinal data.Â dlordinal offers a comprehensive solution for software developers looking to leverage advanced ordinalÂ deep learning capabilities in their applications.

## ðŸ“°Â **NEWS & EDITORIALS**

### **Open Source AI Is the Path Forward**

Read time: 12 minutes

[This article]("https://about-fb-com.cdn.ampproject.org/c/s/about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/amp/") from Meta argues that open-source AI,Â exemplified by the Llama 3.1 model,Â is the best choice for software developers to leverageÂ the latest advancements in generative AI.Â It highlights the benefits of open source,Â including customization,Â portability,Â data privacy,Â and cost-efficiency,Â as well as Meta's commitment to growing the open-source AI ecosystemÂ through partnerships and tooling support.Â The article also discusses the safety and security advantages of open-sourceÂ AI over closed models.

### **Large Enough: Mistral's Latest Generative AI Model**

Read Time: 8 minutes



[Mistral's latest Mistral Large 2 model]("https://mistral.ai/news/mistral-large-2407/")Â offers software developers aÂ powerful AI tool with enhanced performance,Â reasoning,Â and language capabilities.Â The model boasts a 128k contextÂ window,Â supports dozens of languages,Â and delivers state-of-the-art results on code generation and mathematicalÂ benchmarks.Â Developers can access Mistral Large 2 through la Plateforme,Â cloud service providers,Â and open-sourceÂ releases,Â helping them build innovative AI-powered applications.

### **Introducing Llama 3.1: Meta's Most Capable Open-Source Models Yet**

Read Time: 9 minutes



[Meta has released Llama 3.1]("https://ai.meta.com/blog/meta-llama-3-1/"), an open-source large language model they believe rivals top AI models in capabilities for general knowledge, math, coding, and more. The flagship 405B model,Â along with upgraded 8B and 70B versions,Â offer enhanced context length,Â multilingual support,Â and advanced features like synthetic data generation and model distillation.Â Meta is also providing a reference systemÂ and new safety tools to help developers build responsibly with Llama 3.1,Â which is now available on platforms like AWS,Â Azure,Â and Google Cloud.

**Thanks for reading and we will see you next time**

Follow me on [twitter]("https://twitter.com/samkeen"), DM me links you would like included in a future newsletter.
