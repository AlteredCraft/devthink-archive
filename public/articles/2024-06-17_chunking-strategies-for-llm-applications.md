---
title: Chunking Strategies for LLM Applications
author: Sam Keen
date: June 17, 2024
url: https://devthink.ai/p/chunking-strategies-for-llm-applications
scraped_at: 2025-07-29T19:23:30.707473
---

# Chunking Strategies for LLM Applications

*By Sam Keen on June 17, 2024*

---

### **Essential AI Content for Software Devs,** **Minus the Hype**

Hello and thank you for your continued support! This week's edition is packed with valuable insights that will help you stay ahead of the curve in the world of generative AI. Don't miss the articles on building a powerful $300 AI computer, the importance of chunking in RAG applications, and the introduction to LLM agents using Langchain. Plus, we've got the latest news on exciting AI developments, including a new $1 million competition for open AGI progress. Let's dive in!



## ðŸ“–Â **TUTORIALS & CASE STUDIES**

### **How to Build a $300 AI Computer for the GPU-Poor**

Read time: 11 minutes

This article demonstrates how software developers can build a powerfulÂ [$300 AI computer]("https://hackernoon.com/how-to-build-a-$300-ai-computer-for-the-gpu-poor") using an AMD-based mini PC.Â ItÂ covers prepping the system with the right drivers and middleware,Â running large language models like LLaMA 2 on the CPU,Â and generating images with Stable Diffusion on the integrated AMD GPU.Â The key is optimizing memory allocation andÂ leveraging open-source tools like llama.cpp to enable generative AI capabilities on a budget-friendly system.

### **Breaking up is hard to do: Chunking in RAG applications**

Read time: 7 minutes

[This article]("https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/")Â exploresÂ the importance of "chunking" data when building Retrieval Augmented Generation (RAG) systems.Â The author discussesÂ various chunking strategies,Â such as fixed sizes,Â random chunks,Â and context-aware chunking,Â and how they impact theÂ performance of LLM-based applications.Â The key takeaway is that choosing the right chunking approach is crucial forÂ creating accurate and relevant responses grounded in the source data.

### **Chunking Strategies for LLM Applications**

Read time: 10 minutes

[Another article on chunking]("https://www.pinecone.io/learn/chunking-strategies/"), this time from Pinecone, explores the importance of "chunking" -Â breaking downÂ large text into smaller segmentsÂ -Â when building applications that leverage Large Language Models (LLMs).Â It discussesÂ how chunking can optimize the relevance of content retrieved from a vector database,Â and covers various chunkingÂ methods,Â including fixed-size,Â sentence-based,Â and semantic chunking.Â The article provides guidance on determining theÂ optimal chunk size and method for your specific application needs,Â such as semantic search,Â conversational agents,Â orÂ integrating with LLMs with token limits.

### **Intro to LLM Agents with Langchain: When RAG is Not Enough**

Read time: 15 minutes (+ 1 hour video)



This article provides a detailed introductionÂ toÂ [building LLM agents using Langchain]("https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834"),Â a platform for productionizing LLM applications.Â It covers key agent architecture components like planning,Â memory,Â andÂ tools,Â demonstrating how to leverage techniques like chain of thought,Â self-consistency,Â and custom tools.Â The authorÂ showcases how to construct a complete agent that can solve tasks like searching for information and generating CSVÂ files,Â highlighting the importance of integrating all architectural elements for effective agent performance.

### **Retrieval Augmented Generation (RAG) from Scratch**

Read time: 20 minutes (Jupyter notebook)



This tutorial walks through building a Retrieval Augmented Generation (RAG) system from scratch using Python and theÂ Hugging Face Transformers library.Â RAG is a powerful framework for leveraging large language models and knowledge basesÂ to generate high-quality,Â fact-based responses.Â The tutorial covers key concepts like vector databases,Â retrieval,Â andÂ integration with a language model,Â providing a hands-on introduction to this cutting-edge AIÂ technique. **It does NOT use Langchain or Llamaindex**, so you will come out with a deeper understanding the the mechanics of the system vs using the afore mentioned abstraction frameworks. Â [Read the full tutorial here]("https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/RAG-from-Scratch/RAG_from_Scratch.ipynb").

### **ðŸ’°Cost of Self-Hosting Llama-3 8B-Instruct**

Read Time: 10 minutes

This article explores the costÂ ofÂ [self-hosting the Llama-3 8B-Instruct language model]("https://blog.lytix.co/posts/self-hosting-llama-3"),Â comparing it to the pricing of ChatGPT.Â It covers the hardware requirements,Â implementation challenges,Â and anÂ unconventional approach of self-hosting the hardware to significantly reduce the cost per 1 million tokens,Â fromÂ $17Â with a cloud provider to less thanÂ $0.01.Â However,Â the article notes that the breakeven point for this approach isÂ around 5.5 years,Â making it more suitable for high-usage scenarios.

##

## ðŸ§°Â **TOOLS**

### **LSP-AI: An Open-Source Language Server for AI-Powered Programming Assistance**

Read time: 6 minutes

[LSP-AI]("https://github.com/SilasMarvin/lsp-ai")Â is an open-source language server that provides AI-poweredÂ functionality,Â including code completion,Â to software developers through their favorite editors like VS Code,Â Neovim,Â and Emacs.Â LSP-AI aims to assist and empower engineers by integrating AI tools into their existing workflows,Â ratherÂ than replacing them.Â Key features include unified AI features,Â simplified plugin development,Â broad editorÂ compatibility,Â and flexible support for various language models.

### **CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society**

Read Time: 7 minutes



[CAMEL]("https://github.com/camel-ai/camel")Â is an open-source library for studying autonomous and communicative agentsÂ built on large language models.Â It introduces a novel "role-playing" framework to facilitate autonomous cooperationÂ among agents and provide insights into their "cognitive" processes.Â CAMEL enables software developers to generateÂ conversational data for investigating the behaviors and capabilities of chat-based AI agents,Â aiming to advance researchÂ on multi-agent systems and conversational language models.

### **Parakeet, an Easy Way to Create GenAI Applications with Ollama and Golang**

Read Time: 8 minutes

[This article]("https://k33g.hashnode.dev/parakeet-an-easy-way-to-create-genai-applications-with-ollama-and-golang")Â introduces Parakeet,Â a Go library that simplifies the use of the Ollama API for building Generative AI applications.Â Parakeet provides abstractions for managing conversation memory,Â giving instructions to models,Â and adding context,Â making it easy for Go developers to leverage LLMs like Ollama's TinyDolphin.Â The article demonstrates Parakeet's "HelloÂ World" completion example and shows how to build a conversational chat with memory and context,Â highlighting Parakeet'sÂ developer-friendly approach to working with Generative AI.

### **Autoregressive Model Beats Diffusion:** ðŸ¦™ **Llama for Scalable Image Generation**

Read time: 6 minutes

[LlamaGen]("https://github.com/FoundationVision/LlamaGen")Â is a new family of image generation models that apply theÂ next-token prediction paradigm of large language models to the visual generation domain.Â It demonstrates that vanillaÂ autoregressive models like Llama can achieve state-of-the-art image generation performance without specialized visualÂ inductive biases.Â The repository releases pre-trained class-conditional and text-conditional generation models rangingÂ from 100M to 3B parameters,Â along with online demos and a serving framework for high-throughput inference.

### ðŸ¦€Â **Blazingly Fast LLM Inference with Mistral.rs**

Read Time: 8 minutes

[Mistral.rs]("https://github.com/EricLBuehler/mistral.rs")Â is a fast and powerful LLM inference platform for softwareÂ developers,Â supporting a variety of models,Â quantization,Â and easy-to-use APIs in Rust and Python.Â It offersÂ blazing-fast performance,Â accelerator support,Â and advanced features like X-LoRA,Â dynamic adapter swapping,Â andÂ speculative decoding.Â Mistral.rs aims to help developers leverage cutting-edge generative AI capabilities in theirÂ applications.

## ðŸ“°Â **NEWS & EDITORIALS**

### **Announcing ARC Prize: A $1,000,000+ Competition Towards Open AGI Progress**

Read time: 9 minutes

[ARC Prize]("https://arcprize.org/blog/launch")Â is a new competition to solve the ARC-AGI benchmark,Â a measure of generalÂ intelligence that current AI systems struggle with.Â The competition aims to incentivize new ideas in AI research andÂ open-source solutions to push the boundaries of artificial general intelligence (AGI).Â It highlights how closed-sourceÂ progress and a focus on scaling large language models may be hindering true AI advancements.

### **Nvidia's 'Nemotron-4 340B' Model Redefines Synthetic Data Generation, Rivals GPT-4**

Read Time: 8 minutes

[Nvidia's new Nemotron-4 340B model]("https://venturebeat.com/ai/nvidias-nemotron-4-340b-model-redefines-synthetic-data-generation-rivals-gpt-4/")Â is a groundbreaking family of open models that can generate high-quality synthetic data to train large language models.Â With unmatched performance and a commercially-friendly license,Â Nemotron-4 340B empowers businesses to create custom AIÂ models tailored to their needs,Â potentially revolutionizing industries from healthcare to finance.Â This developmentÂ showcases Nvidia's leadership in AI and the growing competition in the chip market as the company strives to stay aheadÂ of tech giants.

### **Technique Improves Reasoning Capabilities of Large Language Models**

Read time: 8 minutes

Researchers have developed a new techniqueÂ calledÂ ["natural language embedded programs" (NLEPs)]("https://news.mit.edu/2024/technique-improves-reasoning-capabilities-large-language-models-0614")Â that enables large language models like GPT-4 to perform complex reasoning tasks more accurately and transparently.Â NLEPs prompt the model to generate a step-by-step Python program to solve a query,Â enhancing its ability to handleÂ numerical,Â symbolic,Â and natural language reasoning.Â This approach offers advantages like improved data privacy and theÂ ability to reuse one program for multiple tasks.

### **LiveBench: A Challenging, Contamination-Free LLM Benchmark**

Read time: 7 minutes



[LiveBench]("https://livebench.ai/")Â is a new benchmark for evaluating Large Language Models (LLMs) that aims to addressÂ issues of test set contamination and objective evaluation.Â LiveBench releases new questions monthly,Â drawing from recentÂ datasets,Â papers,Â news,Â and more,Â ensuring LLMs cannot simply memorize the test set.Â Each question has verifiableÂ ground-truth answers,Â allowing for accurate,Â automated scoring.Â This diverse,Â challenging benchmark will help softwareÂ developers building applications with LLMs and Retrieval Augmented Generation frameworks stay up-to-date on the latestÂ capabilities.

**Thanks for reading and we will see you next time**

Follow me on [twitter]("https://twitter.com/samkeen"), DM me links you would like included in a future newsletters.
